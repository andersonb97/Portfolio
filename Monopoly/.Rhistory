measures.df <- cbind(combinations, measures.df)
toc()
tic()
measures.df <- data.frame()
n.reps <- 20
# The for loop is useful here to iterate through
# all possible combinations of k, n, rho, theta
# and tau2
for (i in 1:dim(combinations)[1]){
rep.data <- replicate(n.reps, {
# in the simulation we first generate a meta dataset
# and we do this by running the meta.data function
meta.data(combinations[i, ])
meta.for <- escalc(measure = "OR",
ai = e.t., bi = t.t. - e.t.,
ci = e.c., di = t.c. - e.c.,
data = sim.data,
append = TRUE)
results <- rma(yi, vi, mods = ~ covar, method = "DL", data = meta.for)
# The output vector returns the p.value associated
# with the test. The second value returned is for
# used to calculate whether the true value is within
# the confidence interval.
if (length(results$pval) == 2){
output <- c(results$beta[2],
results$pval[2],
between(combinations[i, 3], results$ci.lb[2], results$ci.ub[2]),
results$ci.lb[2],
results$ci.ub[2])
output
} else {
output <- c(results$beta[1],
results$pval[1],
between(combinations[i, 3], results$ci.lb[1], results$ci.ub[1]),
results$ci.lb[1],
results$ci.ub[1])
output
}
}
)
rep.data <- as.data.frame(t(rep.data))
colnames(rep.data) <- c("coefficient", "p", "cover", "lower", "upper")
# From the values returned from the replicate function
# we can calculate all of the metrics that we need and
# returns a vector "measures" which is then added to
# the measures.df data.frame
if (combinations[i, 3] == 0){
type.i.error <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- mean(rep.data$cover)
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
power <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
} else {
power <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- sum(rep.data$cover) / dim(rep.data)[1]
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
type.i.error <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
}
measures.df <- rbind(measures.df, measures)
}
# This combines the metrics with the combinations d.f.
measures.df <- cbind(combinations, measures.df)
toc()
library(purrr)
library(metafor)
library(tidyverse)
library(tictoc)
# These are the different parameters that vary from
# combination to combination.
# K is the number of studies in each meta-regression analysis
k <- c(5, 10, 20, 40, 80)
# n is a list of vectors that detail the number of participants. An index is referenced later on which selects which vector in the list will be used. The vectors repeat themselves for studies with k > 5.
n <- list(c(6, 8, 9, 10, 42),
c(16, 18, 19, 20, 52),
c(41, 43, 44, 45, 77))
# rho is the assigned value of the covariate.
rho <- c(0, 0.2, 0.5)
# tau2 is the level of heterogeneity assigned to each meta-regression analysis
tau2 <- c(0, 0.07, 0.32)
# mu (beta in the previous simulation) will be set to 0 with no variation unlike
# in Zabriskie's simulations with mu of -3, -4 and normally distributed with a
# variance of 0.5
mu <- 0
# theta is one of the main differences between the two simulations, with theta
# we can simulate events for both treatment and control groups
theta <- c(0, 0.5, 1, 1.5)
# This is how we know the total number of combinations
# that we need.
number <- length(k) * length(n) * length(rho) * length(tau2) * length(theta)
k_i <- rep(k, each = number/length(k))
n_s <- rep(1:length(n), each = number/length(k)/length(n))
n_i <- rep(n_s, length.out = number)
rho_s <- rep(rho, each = length(n_s)/length(n)/length(rho))
rho_i <- rep(rho_s, length.out = number)
theta_s <- rep(theta, each = length(rho_s)/length(rho)/length(theta))
theta_i <- rep(theta_s, length.out = number)
tau2_i <- rep(tau2, length.out = number)
# the final result is the combinations dataframe which lists out all possible combinations of k, n, rho, theta and tau2
combinations <- cbind(k_i, n_i, rho_i, theta_i, tau2_i)
# Thetas are based off the formula \pi_i = \mu + \theta + \rho + u_i
# This function is fed with a vector as it's argument. The vector is from combinations[i, ] and contains values for k, n, rho, theta and tau2
event.rates <- function(x){
# mu is set to 0 for all simulations
mu_i <<- 0
# value rho grabbed from combinations dataframe
rho_i <<- unname(x[3])
# value theta grabbed from combinations dataframe
theta_s <<- unname(x[4])
# value tau2 grabbed from combinations dataframe
tau2_i <<- unname(x[5])
# here theta_err is modeled around theta but we introduce some variability
theta_err <- rnorm(1, 0, sqrt(tau2_i))
theta_i <<- theta_s + theta_err
# these two formulas will calculate the event rates of control and treatment
# groups for the female covariate value "0"
rate_c <<- exp(mu_i) / (1 + exp(mu_i))
rate_t <<- exp(mu_i + theta_i) / (1 + exp(mu_i + theta_i))
# these two formulas will calculate the event rates of control and treatment
# groups for the male covariate value "1"
rate_c_1 <<- exp(mu_i) / (1 + exp(mu_i))
rate_t_1 <<- exp(mu_i + theta_i + rho_i) / (1 + exp(mu_i + theta_i + rho_i))
}
# in this function I want to enerate a full meta-data set using the event.rates
# used before
meta.data <- function(x){
# calculates the event rates values
event.rates(x)
k_i <<- unname(x[1])
# value n_vec grabbed from combinations dataframe
n_vec <<- unname(n[[x[2]]])
# value n_vec repeats for the length of k
n_vec_i <<- rep(n_vec, length.out = k_i)
covar <<- rbernoulli(k_i, 0.5)
# Now that we have event rates for one randomly simulated
# study, we can calculate the number of events in each of
# the k studies that we are are simulating
female <<- which(covar == FALSE)
male <<- which(covar == TRUE)
sim.data <- data.frame()
if(length(female) > 0 && length(male) > 0){
events_c_0 <<- sapply(female, function(c) {rbinom(1, n_vec_i[c], rate_c)})
events_t_0 <<- sapply(female, function(c) {rbinom(1, n_vec_i[c], rate_t)})
events_c_1 <<- sapply(male, function(c) {rbinom(1, n_vec_i[c], rate_c_1)})
events_t_1 <<- sapply(male, function(c) {rbinom(1, n_vec_i[c], rate_t_1)})
# Now we have data for the number of events in the treatment
# and the number of events in the control.
# We can then rearrange the data and work with it to preform
# meta-analysis and meta-regression (provided that we
# simulate covariate values)
events_t <<- rep(0, k_i)
events_t[female] <<- events_t_0
events_t[male] <<- events_t_1
events_c <<- rep(0, k_i)
events_c[female] <<- events_c_0
events_c[male] <<- events_c_1
# this puts together a dataframe with events for control
# and treatment groups that we can use to run a meta-
# regression analysis
sim.data <<- data.frame("trial" = 1:k_i, "e.t." = events_t,
"t.t." = n_vec_i, "e.c." = events_c,
"t.c." = n_vec_i, "covar" = covar)
}
}
tic()
measures.df <- data.frame()
n.reps <- 20
# The for loop is useful here to iterate through
# all possible combinations of k, n, rho, theta
# and tau2
for (i in 1:dim(combinations)[1]){
rep.data <- replicate(n.reps, {
# in the simulation we first generate a meta dataset
# and we do this by running the meta.data function
meta.data(combinations[i, ])
meta.for <- escalc(measure = "OR",
ai = e.t., bi = t.t. - e.t.,
ci = e.c., di = t.c. - e.c.,
data = sim.data,
append = TRUE)
results <- rma(yi, vi, mods = ~ covar, method = "DL", data = meta.for)
# The output vector returns the p.value associated
# with the test. The second value returned is for
# used to calculate whether the true value is within
# the confidence interval.
if (length(results$pval) == 2){
output <- c(results$beta[2],
results$pval[2],
between(combinations[i, 3], results$ci.lb[2], results$ci.ub[2]),
results$ci.lb[2],
results$ci.ub[2])
output
} else {
output <- c(results$beta[1],
results$pval[1],
between(combinations[i, 3], results$ci.lb[1], results$ci.ub[1]),
results$ci.lb[1],
results$ci.ub[1])
output
}
}
)
rep.data <- as.data.frame(t(rep.data))
colnames(rep.data) <- c("coefficient", "p", "cover", "lower", "upper")
# From the values returned from the replicate function
# we can calculate all of the metrics that we need and
# returns a vector "measures" which is then added to
# the measures.df data.frame
if (combinations[i, 3] == 0){
type.i.error <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- mean(rep.data$cover)
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
power <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
} else {
power <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- sum(rep.data$cover) / dim(rep.data)[1]
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
type.i.error <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
}
measures.df <- rbind(measures.df, measures)
}
# This combines the metrics with the combinations d.f.
measures.df <- cbind(combinations, measures.df)
toc()
coeff.values <- replicate(n.reps, {
# in the simulation we first generate a meta dataset
# and we do this by running the meta.data function
meta.data(combinations[i, ])
meta.for <- escalc(measure = "OR",
ai = e.t., bi = t.t. - e.t.,
ci = e.c., di = t.c. - e.c.,
data = sim.data,
append = TRUE)
results <- rma(yi, vi, mods = ~ covar, method = "DL", data = meta.for)
# The output vector returns the p.value associated
# with the test. The second value returned is for
# used to calculate whether the true value is within
# the confidence interval.
results$beta[2]
}
)
# These are the different parameters that vary from
# combination to combination.
# K is the number of studies in each meta-regression analysis
k <- c(5, 10, 20, 40, 80)
# n is a list of vectors that detail the number of participants. An index is referenced later on which selects which vector in the list will be used. The vectors repeat themselves for studies with k > 5.
n <- list(c(6, 8, 9, 10, 42),
c(16, 18, 19, 20, 52),
c(41, 43, 44, 45, 77))
# rho is the assigned value of the covariate.
rho <- c(0, 1.5, 3)
# tau2 is the level of heterogeneity assigned to each meta-regression analysis
tau2 <- c(0, 0.07, 0.32)
# mu (beta in the previous simulation) will be set to 0 with no variation unlike
# in Zabriskie's simulations with mu of -3, -4 and normally distributed with a
# variance of 0.5
mu <- 0
# theta is one of the main differences between the two simulations, with theta
# we can simulate events for both treatment and control groups
theta <- c(0, 0.5, 1, 1.5)
# This is how we know the total number of combinations
# that we need.
number <- length(k) * length(n) * length(rho) * length(tau2) * length(theta)
k_i <- rep(k, each = number/length(k))
n_s <- rep(1:length(n), each = number/length(k)/length(n))
n_i <- rep(n_s, length.out = number)
rho_s <- rep(rho, each = length(n_s)/length(n)/length(rho))
rho_i <- rep(rho_s, length.out = number)
theta_s <- rep(theta, each = length(rho_s)/length(rho)/length(theta))
theta_i <- rep(theta_s, length.out = number)
tau2_i <- rep(tau2, length.out = number)
# the final result is the combinations dataframe which lists out all possible combinations of k, n, rho, theta and tau2
combinations <- cbind(k_i, n_i, rho_i, theta_i, tau2_i)
# These are the different parameters that vary from
# combination to combination.
# K is the number of studies in each meta-regression analysis
k <- c(5, 10, 20, 40, 80)
# n is a list of vectors that detail the number of participants. An index is referenced later on which selects which vector in the list will be used. The vectors repeat themselves for studies with k > 5.
n <- list(c(6, 8, 9, 10, 42),
c(16, 18, 19, 20, 52),
c(41, 43, 44, 45, 77))
# rho is the assigned value of the covariate.
rho <- c(0, 1.5, 3)
# tau2 is the level of heterogeneity assigned to each meta-regression analysis
tau2 <- c(0, 0.07, 0.32)
# mu (beta in the previous simulation) will be set to 0 with no variation unlike
# in Zabriskie's simulations with mu of -3, -4 and normally distributed with a
# variance of 0.5
mu <- 0
# theta is one of the main differences between the two simulations, with theta
# we can simulate events for both treatment and control groups
theta <- c(0, 0.5, 1, 1.5)
# This is how we know the total number of combinations
# that we need.
number <- length(k) * length(n) * length(rho) * length(tau2) * length(theta)
k_i <- rep(k, each = number/length(k))
n_s <- rep(1:length(n), each = number/length(k)/length(n))
n_i <- rep(n_s, length.out = number)
rho_s <- rep(rho, each = length(n_s)/length(n)/length(rho))
rho_i <- rep(rho_s, length.out = number)
theta_s <- rep(theta, each = length(rho_s)/length(rho)/length(theta))
theta_i <- rep(theta_s, length.out = number)
tau2_i <- rep(tau2, length.out = number)
# the final result is the combinations dataframe which lists out all possible combinations of k, n, rho, theta and tau2
combinations <- cbind(k_i, n_i, rho_i, theta_i, tau2_i)
# Thetas are based off the formula \pi_i = \mu + \theta + \rho + u_i
# This function is fed with a vector as it's argument. The vector is from combinations[i, ] and contains values for k, n, rho, theta and tau2
event.rates <- function(x){
# mu is set to 0 for all simulations
mu_i <<- 0
# value rho grabbed from combinations dataframe
rho_i <<- unname(x[3])
# value theta grabbed from combinations dataframe
theta_s <<- unname(x[4])
# value tau2 grabbed from combinations dataframe
tau2_i <<- unname(x[5])
# here theta_err is modeled around theta but we introduce some variability
theta_err <- rnorm(1, 0, sqrt(tau2_i))
theta_i <<- theta_s + theta_err
# these two formulas will calculate the event rates of control and treatment
# groups for the female covariate value "0"
rate_c <<- exp(mu_i) / (1 + exp(mu_i))
rate_t <<- exp(mu_i + theta_i) / (1 + exp(mu_i + theta_i))
# these two formulas will calculate the event rates of control and treatment
# groups for the male covariate value "1"
rate_c_1 <<- exp(mu_i) / (1 + exp(mu_i))
rate_t_1 <<- exp(mu_i + theta_i + rho_i) / (1 + exp(mu_i + theta_i + rho_i))
}
# in this function I want to enerate a full meta-data set using the event.rates
# used before
meta.data <- function(x){
# calculates the event rates values
event.rates(x)
k_i <<- unname(x[1])
# value n_vec grabbed from combinations dataframe
n_vec <<- unname(n[[x[2]]])
# value n_vec repeats for the length of k
n_vec_i <<- rep(n_vec, length.out = k_i)
covar <<- rbernoulli(k_i, 0.5)
# Now that we have event rates for one randomly simulated
# study, we can calculate the number of events in each of
# the k studies that we are are simulating
female <<- which(covar == FALSE)
male <<- which(covar == TRUE)
sim.data <- data.frame()
if(length(female) > 0 && length(male) > 0){
events_c_0 <<- sapply(female, function(c) {rbinom(1, n_vec_i[c], rate_c)})
events_t_0 <<- sapply(female, function(c) {rbinom(1, n_vec_i[c], rate_t)})
events_c_1 <<- sapply(male, function(c) {rbinom(1, n_vec_i[c], rate_c_1)})
events_t_1 <<- sapply(male, function(c) {rbinom(1, n_vec_i[c], rate_t_1)})
# Now we have data for the number of events in the treatment
# and the number of events in the control.
# We can then rearrange the data and work with it to preform
# meta-analysis and meta-regression (provided that we
# simulate covariate values)
events_t <<- rep(0, k_i)
events_t[female] <<- events_t_0
events_t[male] <<- events_t_1
events_c <<- rep(0, k_i)
events_c[female] <<- events_c_0
events_c[male] <<- events_c_1
# this puts together a dataframe with events for control
# and treatment groups that we can use to run a meta-
# regression analysis
sim.data <<- data.frame("trial" = 1:k_i, "e.t." = events_t,
"t.t." = n_vec_i, "e.c." = events_c,
"t.c." = n_vec_i, "covar" = covar)
}
}
tic()
measures.df <- data.frame()
n.reps <- 20
# The for loop is useful here to iterate through
# all possible combinations of k, n, rho, theta
# and tau2
for (i in 1:dim(combinations)[1]){
rep.data <- replicate(n.reps, {
# in the simulation we first generate a meta dataset
# and we do this by running the meta.data function
meta.data(combinations[i, ])
meta.for <- escalc(measure = "OR",
ai = e.t., bi = t.t. - e.t.,
ci = e.c., di = t.c. - e.c.,
data = sim.data,
append = TRUE)
results <- rma(yi, vi, mods = ~ covar, method = "DL", data = meta.for)
# The output vector returns the p.value associated
# with the test. The second value returned is for
# used to calculate whether the true value is within
# the confidence interval.
if (length(results$pval) == 2){
output <- c(results$beta[2],
results$pval[2],
between(combinations[i, 3], results$ci.lb[2], results$ci.ub[2]),
results$ci.lb[2],
results$ci.ub[2])
output
} else {
output <- c(results$beta[1],
results$pval[1],
between(combinations[i, 3], results$ci.lb[1], results$ci.ub[1]),
results$ci.lb[1],
results$ci.ub[1])
output
}
}
)
rep.data <- as.data.frame(t(rep.data))
colnames(rep.data) <- c("coefficient", "p", "cover", "lower", "upper")
# From the values returned from the replicate function
# we can calculate all of the metrics that we need and
# returns a vector "measures" which is then added to
# the measures.df data.frame
if (combinations[i, 3] == 0){
type.i.error <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- mean(rep.data$cover)
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
power <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
} else {
power <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- sum(rep.data$cover) / dim(rep.data)[1]
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
type.i.error <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
}
measures.df <- rbind(measures.df, measures)
}
# This combines the metrics with the combinations d.f.
measures.df <- cbind(combinations, measures.df)
toc()
tic()
measures.df <- data.frame()
n.reps <- 100
# The for loop is useful here to iterate through
# all possible combinations of k, n, rho, theta
# and tau2
for (i in 1:dim(combinations)[1]){
rep.data <- replicate(n.reps, {
# in the simulation we first generate a meta dataset
# and we do this by running the meta.data function
meta.data(combinations[i, ])
meta.for <- escalc(measure = "OR",
ai = e.t., bi = t.t. - e.t.,
ci = e.c., di = t.c. - e.c.,
data = sim.data,
append = TRUE)
results <- rma(yi, vi, mods = ~ covar, method = "DL", data = meta.for)
# The output vector returns the p.value associated
# with the test. The second value returned is for
# used to calculate whether the true value is within
# the confidence interval.
if (length(results$pval) == 2){
output <- c(results$beta[2],
results$pval[2],
between(combinations[i, 3], results$ci.lb[2], results$ci.ub[2]),
results$ci.lb[2],
results$ci.ub[2])
output
} else {
output <- c(results$beta[1],
results$pval[1],
between(combinations[i, 3], results$ci.lb[1], results$ci.ub[1]),
results$ci.lb[1],
results$ci.ub[1])
output
}
}
)
rep.data <- as.data.frame(t(rep.data))
colnames(rep.data) <- c("coefficient", "p", "cover", "lower", "upper")
# From the values returned from the replicate function
# we can calculate all of the metrics that we need and
# returns a vector "measures" which is then added to
# the measures.df data.frame
if (combinations[i, 3] == 0){
type.i.error <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- mean(rep.data$cover)
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
power <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
} else {
power <- sum(rep.data[, 2] < 0.05) / n.reps
bias <- mean(rep.data[, 1]) - unname(combinations[i, 3])
coverage <- sum(rep.data$cover) / dim(rep.data)[1]
upper <- mean(rep.data$upper)
lower <- mean(rep.data$lower)
type.i.error <- NA
measures <- cbind(type.i.error, power, bias, coverage, lower, upper)
}
measures.df <- rbind(measures.df, measures)
}
# This combines the metrics with the combinations d.f.
measures.df <- cbind(combinations, measures.df)
toc()
save.image("C:/Ben's Stuff/BYU/SpringSummer 2020/Meta Regression Research/Covariate Simulation/Covariate Simulation Code (Combinations) Environment 7.30.RData")
